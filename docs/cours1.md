# 1. Intro Big Data 

### 1.1 D√©finition & Enjeux: 

**A. D√©finition Big Data**: Big Data, ou "m√©gadonn√©es" en fran√ßais, d√©signe des ensembles de donn√©es extr√™mement volumineux, complexes et en croissance rapide, qui d√©passent les capacit√©s des outils classiques de gestion de bases de donn√©es pour les capturer, stocker, g√©rer et analyser dans un temps raisonnable.

| Name           | Value (10^) | Value (2^) |
| -------------- | ----------- | ---------- |
| kilobyte (kB)  | 10^3        | 2^10       |
| megabyte (MB)  | 10^6        | 2^20       |
| gigabyte (GB)  | 10^9        | 2^30       |
| terabyte (TB)  | 10^12       | 2^40       |
| petabyte (PB)  | 10^15       | 2^50       |
| exabyte (EB)   | 10^18       | 2^60       |
| zettabyte (ZB) | 10^21       | 2^70       |
| yottabyte (YB) | 10^24       | 2^80       |

Quelques statistiques int√©ressantes :
- Plus de 1 exaoctet de donn√©es est g√©n√©r√© chaque jour.
- Une personne moyenne g√©n√®re 1,7 Mo de donn√©es par seconde.
- 90 % des donn√©es mondiales ont √©t√© cr√©√©es au cours des deux derni√®res ann√©es.

üìåLes 5 V du Big Data :
- **Volume**: La quantit√© massive de donn√©es g√©n√©r√©es et stock√©es.
- **V√©locit√©**: La vitesse √† laquelle les nouvelles donn√©es sont g√©n√©r√©es et circulent, souvent en temps r√©el ou quasi-r√©el.
- **Vari√©t√©**: La diversit√© des types de donn√©es, structur√©es, semi-structur√©es et non structur√©es, provenant de sources multiples.
- **V√©racit√©**: La fiabilit√© et la qualit√© des donn√©es, qui peuvent √™tre incertaines ou impr√©cises et n√©cessitent une validation.
- **Valeur**: La capacit√© √† transformer ces vastes ensembles de donn√©es en informations utiles et exploitables pour l'entreprise.

![alt text](img/cours1/5v_of_big_data.png)


**B. Les Enjeux du Big Data**:
"You can have data without information, but you **can't have information without data**"

La donn√©e est omnipr√©sente et sa ma√Ætrise est cruciale pour faciliter la prise de d√©cision au sein des entreprises.

Le Big Data pr√©sente √† la fois des opportunit√©s consid√©rables et des d√©fis complexes. 
D'un c√¥t√©, il offre la possibilit√© d'exploiter des volumes massifs d'informations pour gagner en efficacit√©, innover et cr√©er de la valeur. 

De l'autre, sa gestion comporte de nombreux **d√©fis**: 
- **La lutte contre la "myopie des donn√©es"** : Les organisations peinent souvent √† localiser et exploiter efficacement leurs propres ressources informationnelles, limitant ainsi leur capacit√© √† tirer pleinement parti de leurs donn√©es.
- **La ma√Ætrise du cycle complet de la donn√©e** : De la collecte √† l'analyse et la restitution, en passant par le stockage et la s√©curisation, les entreprises doivent g√©rer efficacement chaque √©tape du parcours de la donn√©e.
- **La gestion de la diversit√© organisationnelle** : La complexit√© s'accro√Æt avec la multiplicit√© des entit√©s r√©parties √† travers le monde et les diff√©rents fuseaux horaires, rendant la coordination et l'uniformisation des pratiques de gestion des donn√©es plus difficiles.
- **Les choix technologiques strat√©giques** : Les d√©cisions en mati√®re de technologies doivent prendre en compte la disponibilit√© des ressources techniques tout en s'adaptant √† l'inertie et √† la flexibilit√© des syst√®mes d'information existants.
- **La garantie de la s√©curit√© et de la conformit√©** : Les entreprises doivent assurer la s√©curit√© et la confidentialit√© des donn√©es tout en respectant les r√©glementations en vigueur, ajoutant une couche suppl√©mentaire de complexit√© √† leur strat√©gie de gestion des donn√©es.
- **L'adaptation √† un environnement en constante √©volution** : Les entreprises doivent rester √† jour face aux changements rapides du cadre l√©gislatif (par exemple, l'introduction du RGPD en Europe) et aux avanc√©es technologiques continues dans le domaine du traitement des donn√©es.

Le Big Data a r√©volutionn√© le paysage √©conomique, touchant pratiquement tous les secteurs d'activit√©. Son influence s'√©tend bien au-del√† de la technologie, transformant profond√©ment des industries diverses (Pharmaceutique, Automobile, Retail, Energie, Agriculture, Luxe, Banque etc....)


üìå**C. Cycle de vie de la donn√©e:**
La donn√©e √©tant la composante de base du Big Data, il est essentiel de comprendre son cycle de vie. Les ing√©nieurs en Big Data seront amen√© √† intervenir √† chaque √©tape de ce cycle.

-1. G√©n√©ration/Carthographie des sources > 2. Collecte et acquisition > 3. Stockage > 4. Traitement > 5. management (conformit√©, qualit√©, uniformisation, confidentialit√© etc...) > 6. Analyse > 7. Interpr√©tation et visualisation > 8. Archivage/Suppression

![alt text](img/cours1/data_lifecycle_mgt.PNG)


üìå**D. Familles d'outil :**
A chaque √©tape du parcours de la donn√©e fait intervenir un/des outillage(s) sp√©cifique(s).
On peut classer les outils de Big Data dans les familles suivantes: 
- Outils d'ingestion/collecte de donn√©es: Connecteurs de base de donn√©es (ex: jdbc, odbc ...), Airbyte, Kafka, Spark, Flume, Logstash ...
- Outils de stockage: HDFS, HBase, Cassandra, Amazon s3, OracleDB, MongoDB, Google Cloud Storage, Snowflake etc...
- Outils de traitement et d'analyse (dit processing): Hadoop, Spark, Flink, AWS Glue, Databricks, dbt, Hive etc...
- Outils de visualisation: Tableau, PowerBI, Qlik, Kibana, Grafana etc...
- Outils de traitement ML/AI: SparkML, H2O.ai, DataRobot, MLflow, Kubeflow etc...
- Outils de management: Airflow, Databricks Lifecycle management, Luigi etc...



### 1.2) Quelques concepts cl√©s: 

üìå**A. Concepts g√©n√©raux:**
- **Repr√©sentation binaire:** tous les fichiers, quelle que soit leur nature (image, vid√©o, texte, son etc...), sont en format binaire au niveau le plus bas du stockage. Donc lorsque HDFS ou tout autre programme manipule les fichiers d'entr√©e, ils manipulent la repr√©sentation binaire de ceux-ci.
  
- **Source code, bytecode, machine code**:
	- Le code source est le code √©crit par les d√©veloppeurs dans un langage de programmation comme Java. 
	- Ce code est ensuite compil√© en bytecode, un format interm√©diaire ind√©pendant de la plateforme. 
	- Enfin, le machine/binary code est le code binaire sp√©cifique √† chaque architecture mat√©rielle (g√©n√©r√© par la JVM √† partir du bytecode pour l'ex√©cution finale).

- **La JVM (Java Virtual Machine)**: est un moteur d'ex√©cution multiplateforme qui peut ex√©cuter les instructions compil√©es en bytecode Java. 

![alt text](img/cours1/source_compiled_binary_code.png)


- **Fiabilit√©, Scalabilit√©, Maintenabilit√©** :
	- La fiabilit√© est la capacit√© d'un syst√®me √† fonctionner correctement m√™me en cas de probl√®mes ou de pannes. 
	- La scalabilit√© est l'aptitude d'un syst√®me √† s'adapter √† une charge croissante. 
	- La maintenabilit√© concerne la facilit√© avec laquelle un syst√®me peut √™tre modifi√©, am√©lior√© ou corrig√© au fil du temps.

- **R√©plication**: 
	La **r√©plication** consiste √† copier et maintenir des donn√©es sur plusieurs n≈ìuds pour assurer la disponibilit√© et la tol√©rance aux pannes dans un syst√®me distribu√©.
	Le facteur de r√©plication R standard en Big Data est de: 3 (Le primaire + 2 r√©plicas)
	
	![alt text](img/cours1/replication.PNG)

	- Strat√©gies : 
		- R√©plication **synchrone vs asynchrone** (Maj des r√©plicas synchrone ou asynchrone par rapport au primaire.)
		- R√©plication **active vs passive** (Actif si les r√©plicas sont expos√©s aux utilisateurs de la m√™me mani√®re que le primaire. Passif s'ils servent purement de back-up) 
		  
	- Int√©r√™t en Big Data :
	    - Haute disponibilit√© : les donn√©es restent accessibles m√™me en cas de panne d'un n≈ìud
	    - Tol√©rance aux pannes : pr√©vient la perte de donn√©es en cas de d√©faillance mat√©rielle
	    - √âquilibrage de charge : permet de distribuer les requ√™tes sur plusieurs r√©plicas
	    - Am√©lioration des performances de lecture en multipliant les sources de donn√©es

- **Partitionnement** (pour le stockage de donn√©es):
	Le partitionnement est une technique de division des donn√©es en sous-ensembles plus petits et plus g√©rables, appel√©s partitions. Cela permet une meilleure distribution de la charge et une gestion plus efficace des grandes quantit√©s de donn√©es dans un environnement Big Data. Un partitionnement dit **logique** s‚Äôeffectue en fonction d‚Äôune¬†**_cl√©_** de partitionnement, soit un ou plusieurs attributs dont la valeur sert de crit√®re √† l‚Äôaffectation d‚Äôun document √† un fragment. La premi√®re d√©cision √† prendre est donc le choix de la cl√©. Un bon partitionnement r√©partit les documents en fragments de taille comparable.
	Exemple de cl√©s: par localisation g√©ographique, par ann√©e, par r√©gion/entit√©s etc....

- **Encodage et Format:**
	-L'encodage concerne la repr√©sentation des donn√©es au niveau le plus bas, c'est-√†-dire la fa√ßon dont les caract√®res (texte) ou les valeurs sont convertis en bits.
	- Exemples d'encodages :
	    - UTF-8, UTF-16, ASCII (Latin caracters) pour le texte
	    - IEEE 754 pour les nombres √† virgule flottante
	    - Base64 pour repr√©senter des donn√©es binaires (comprend aussi images et sons) en texte (https://stackoverflow.com/questions/3538021/why-do-we-use-base64)
	- R√¥le :
	    - D√©finit comment chaque √©l√©ment de donn√©e individuel est repr√©sent√© en binaire
	    - Assure que les donn√©es peuvent √™tre correctement interpr√©t√©es par diff√©rents syst√®mes et programmes. (Ex: '\\' en fonction des syst√®mes ce caract√®re n'aura pas la m√™me signification au moment de la lecture d'un script/fichier)
	    
	-Le format, quant √† lui, une mani√®re sp√©cifique d'organiser les informations num√©riques √† un niveau plus √©lev√©. Il d√©termine comment celles-ci vont √™tres lues et trait√©es par un ordinateur, appareil ou programme.
	- Exemples de formats :
	    - JSON, XML, CSV pour les donn√©es structur√©es
	    - Avro, Parquet, ORC pour les donn√©es Big Data
	    - JPEG, PNG pour les images
	- R√¥le :
	    - Organise les donn√©es de mani√®re logique et coh√©rente
	    - Facilite l'√©change et l'interpr√©tation des donn√©es entre diff√©rentes applications
	On distinguera 2 types de formats: 
	- Format binaire:
	    - Donn√©es stock√©es directement en s√©quences de bits
	    - Non lisible directement par l'humain
	    - Optimis√© pour le traitement par machine
	    - Exemples: .jpg, .exe, .mp3
	- Format non binaire (ou texte):
	    - Donn√©es stock√©es en caract√®res lisibles par l'humain
	    - G√©n√©ralement en encodage ASCII ou Unicode
	    - Peut √™tre ouvert et lu dans un √©diteur de texte simple
	    - Exemples: .txt, .csv, .html
	-Relation entre encodage et format :
	- L'encodage est utilis√© au sein d'un format : Par exemple, un fichier JSON (format) peut contenir du texte encod√© en UTF-8 (encodage).
	- Le format peut sp√©cifier l'encodage : Certains formats, comme XML, permettent de d√©clarer l'encodage utilis√© e.g.,
	```xml
	<?xml version="1.0" encoding="UTF-8"?>
	```
	- Compl√©mentarit√© : L'encodage assure que les donn√©es peuvent √™tre correctement lues au niveau binaire.


**B. Concepts li√©s au transfert des donn√©es √† travers le r√©seau:**
- **Archivage**: Regroupement de fichiers en un seul pour faciliter stockage/transfert. Les fichiers .jar (similaire √† des fichiers zip mais sp√©cifique √† java) sont utilis√©s dans Hadoop afin d'encapsuler le code des applications Hadoop, et faciliter le d√©ploiement et l'ex√©cution des jobs sur le cluster. Ces archives contiennent le code et les d√©pendances (classes java compil√©es(bytecode), fichiers de configuration, biblioth√®ques/librairies, fichier manifest etc..) n√©cessaires √† l'ex√©cution du job. 
  
- **Routage et data locality**: 
  -Le routage concerne la mani√®re dont les donn√©es sont dirig√©es (rout√©es) √† travers un r√©seau (cluster de machines). Hadoop s'appuie sur le protocol TCP/IP afin de transporter les donn√©es et jobs d'une machine √† l'autre.

  ![alt text](img/cours1/partition-distrib.png)

  -Data locality: est une *strat√©gie* qui vise √† rapprocher le traitement des donn√©es (jobs/tasks) de leur lieu de stockage pour r√©duire la latence et am√©liorer les performances. 
  Retenez bien, il est plus facile de d√©placer les jobs/t√¢ches plut√¥t que les partitions!

- **S√©rialisation (et d√©s√©rialisation):** Processus de conversion d'objets complexes (classe, un dataframe etc...) en s√©quences d'octets **pour le stockage ou la transmission**, permettant leur reconstruction ult√©rieure (d√©s√©rialisation).
    - Techniques : Java Serialization, Protocol Buffers, Thrift, Avro
    - Int√©r√™t en Big Data :
        - Permet le transfert efficace d'objets complexes (instances de classes, enregistrements, paires cl√©-valeurs, r√©sultats interm√©diaires ...) entre n≈ìuds d'un cluster
        - Facilite le stockage d'objets dans des syst√®mes distribu√©s (ex: HDFS)
        - Optimise la performance en r√©duisant la taille des donn√©es transmises
        - Assure la compatibilit√© entre diff√©rentes versions de donn√©es ou de code
	NB: Hadoop utilise son propre framework de s√©rialisation: Writable (via m√©thode write(): convertit l'objet en bytes et readFields(): reconstruit l'objet √† partir des bytes)


**C. Concepts li√©s au computing:**
- **Le programme, le processus & le daemon:**
	- Un programme: un ensemble d'instructions √©crites par un d√©veloppeur. Il est stock√© sous forme de fichier ex√©cutable sur un support de stockage. Le programme reste passif jusqu'√† son lancement.
	- Un processus: une instance active d'un programme en cours d'ex√©cution. Il occupe de la m√©moire et utilise des ressources du syst√®me. Chaque processus a son propre √©tat qui √©volue au fil du temps. Le syst√®me d'exploitation g√®re les processus. Un m√™me programme peut g√©n√©rer plusieurs processus distincts. Chaque processus poss√®de ses propres ressources allou√©es. Chaque processus peut contenir plusieurs threads. Ces threads sont des unit√©s d'ex√©cution plus l√©g√®res au sein d'un processus. Ils partagent le m√™me espace m√©moire du processus parent.
	- Un daemon: est un programme informatique qui s'ex√©cute en arri√®re-plan, plut√¥t que sous le contr√¥le direct d'un utilisateur interactif.

	![alt text](img/cours1/program_process_threads_diff.png)


- üìå**Single machine, cloud & distributed computing:**
	- Single machine computing (multi-tasks vs multicore parallelism) : 
		- Multi-tasking: la capacit√© de notre machine √† traiter plusieurs t√¢ches li√©es √† des programmes diff√©rents. Le multit√¢che donne l'illusion de parall√©lisme par commutation rapide (en sautant d'une t√¢che √† l'autre).
		- Le parall√©lisme multic≈ìur: ex√©cution en parall√®le et simultan√©e sur diff√©rents c≈ìurs de t√¢ches li√©es √† des programmes diff√©rents. 
	- Cloud computing: pratique consistant √† utiliser un/des serveur(s) informatique(s) √† distance, h√©berg√©(s) dans des centres de donn√©es connect√©s √† Internet pour stocker, g√©rer et traiter des donn√©es, plut√¥t qu'un serveur local ou un ordinateur personnel.
	- Distributed computing : m√©thode qui consiste √† faire travailler ensemble plusieurs ordinateurs en r√©seau afin de r√©soudre un probl√®me commun. Le probl√®me √©tant divis√© en plusieurs t√¢ches, chacune d'elles est r√©solue par un ou plusieurs ordinateurs qui communiquent entre eux.

- üìå**Vertical scaling vs Horizontal scaling:** 
  Le scaling vertical consiste √† augmenter les ressources d'une seule machine, tandis que le scaling horizontal implique l'ajout de machines suppl√©mentaires √† un syst√®me distribu√©.







# 2. Hadoop 

üìå**Hadoop** est un :
- **framework**: 'cadre de travail' compos√© d'un ensemble de biblioth√®ques, d'outils, de conventions de codage, et de mod√®les de conception qui simplifient et acc√©l√®rent le processus de d√©veloppement logiciel
- **open-source**: con√ßu pour √™tre accessible au public, n'importe qui peut voir, modifier et distribuer le code comme il l'entend.
- con√ßu pour le stockage et le traitement distribu√© de grandes quantit√©s de donn√©es (Big Data) sur des clusters de machines ordinaires. 
Il a √©t√© d√©velopp√© par la Apache Software Foundation et est devenu l'un des outils les plus populaires pour g√©rer le Big Data.

Hadoop sert principalement √† :
1. Stocker de grandes quantit√©s de donn√©es de mani√®re distribu√©e et fiable.
2. Traiter ces donn√©es en parall√®le sur plusieurs machines.
3. G√©rer les pannes mat√©rielles de mani√®re transparente.
4. Permettre une scalabilit√© horizontale facile (ajout de nouvelles machines au cluster).


üìåHadoop a une topologie dite "ma√Ætre-esclave"(master-slave en anglais). 
Dans cette topologie, nous avons _un n≈ìud ma√Ætre et plusieurs n≈ìuds esclaves_. 
-La fonction du n≈ìud ma√Ætre (master node) est d'assigner une t√¢che aux diff√©rents n≈ìuds esclaves et de g√©rer les ressources. Il stocke les m√©tadonn√©es, √† savoir les informations relatives aux donn√©es stock√©s (comme l'emplacement des blocs/partitions √† travers le r√©seau de noeuds esclaves).
-Les n≈ìuds esclaves (slave nodes) effectuent le calcul proprement dit. Ils sont aussi ceux qui stockent les donn√©es r√©elles.

![alt text](img/cours1/hadoop_functionning.PNG)


### 2.1) √âcosyst√®me Hadoop
L'√©cosyst√®me Hadoop comprend un ensemble de projets et d'outils compl√©mentaires qui s'int√®grent avec les composants principaux de Hadoop pour fournir des fonctionnalit√©s suppl√©mentaires.

![alt text](img/cours1/hadoop_architecture.PNG)

üìåLes composants prinicipaux de cet √©cosyst√®me: 
‚Äì **HDFS** est utilis√© pour distribuer de grands ensembles de donn√©es
‚Äì YARN (Yet Another Resource Negotiator) : une sorte de syst√®me d'exploitation pour Hadoop. Introduit dans Hadoop 2.0, il g√®re les ressources du cluster, permet l'ex√©cution d'applications distribu√©es autres que MapReduce sur le cluster Hadoop (ex: Spark, Hive, Pig etc...). 
‚Äì **MapReduce**¬†application utilis√©e pour distribuer une t√¢che de calcul √† un ensemble de donn√©es distribu√©es.

![alt text](img/cours1/hadoop_ecosystem.PNG)

Autres composants importants:
- Hive : Permet d'ex√©cuter des requ√™tes SQL-like sur les donn√©es stock√©es dans HDFS.
- Pig : Langage de haut niveau pour l'analyse de donn√©es.
- HBase : Base de donn√©es NoSQL distribu√©e pour le stockage de grandes tables.
- Spark : Framework et moteur de traitement de donn√©es en m√©moire (souvent utilis√© avec Hadoop).
- Oozie : Planificateur de workflows pour les jobs Hadoop.
- Sqoop : Outil pour transf√©rer des donn√©es entre bases de donn√©es relationnelles et Hadoop.
- Flume : Service pour collecter, agr√©ger et d√©placer de grandes quantit√©s de donn√©es de log.



### 2.2) HDFS, MapReduce et YARN

#### 2.2.1) HDFS üìå
HDFS signifie Syst√®me de Fichiers Distribu√©s Hadoop (Hadoop Distributed File System, un 'File System' est une m√©thode et une structure de donn√©es (arborescente) que le syst√®me d'exploitation utilise pour contr√¥ler la mani√®re dont les donn√©es sont stock√©es et r√©cup√©r√©es sur un dispositif de stockage, tel qu'un disque dur, un disque SSD ou m√™me un stockage en nuage.). 
Il assure le **stockage des donn√©es sur Hadoop**. Toutes donn√©e stock√©e est subdivis√©e par HDFS  en unit√©s plus petites appel√©es blocs puis op√®re √† son stockage **sur disque** de mani√®re distribu√©e (ie r√©parti sur plusieurs machines/noeuds). 

Pour fonctionner, HDFS s'appuie sur 2 daemons: 
-L'un qui tourne sur le n≈ìud principal (master node), appel√©: **NameNode**. Il est responsable de la gestion de l'espace de noms et r√©gule l'acc√®s aux fichiers par le client (programme/application qui fait les requ√™tes pour acc√©der aux fichiers). Il s'agit par exemple d'actions telles que l'ouverture, la fermeture et le renommage de fichiers ou de r√©pertoires. Il assure √©galement le suivi de la correspondance entre les blocs de donn√©es (la data) et les n≈ìuds de donn√©es qui les stocke (les machines).

-Et l'autre qui tourne sur les n≈ìuds esclaves (slave nodes), appel√©: **DataNode**. Ils sont responsables du stockage r√©el des donn√©es. Ils r√©pondent aux demandes de lecture/√©criture des clients du syst√®me de fichiers. Le DataNode cr√©e, supprime et r√©plique √©galement des blocs √† la demande du NameNode.

![alt text](img/cours1/name_data_node_architecture.PNG)

Aspects cl√©s du fonctionnement de HDFS:
- üìå**Block Protocol**:
    - HDFS traite tous les fichiers comme des donn√©es binaires. Il ne fait pas de distinction entre les types de fichiers lorsqu'il les divise en blocs. L'application qui lit les donn√©es de HDFS est charg√©e de les interpr√©ter correctement.
    - HDFS divise les fichiers en blocs (typiquement 128 Mo ou 256 Mo). Chaque bloc est r√©pliqu√© sur plusieurs n≈ìuds esclaves (le facteur de r√©plication par d√©faut est de 3 = 1 primaire + 2 r√©pliques).
    
      ![alt text](img/cours1/exemple_block_file.PNG)

- **Heartbeat Protocol**:
    - DataNodes envoie p√©riodiquement des "heartbeats" (signes de vie) au NameNode.
    - Cela aide le NameNode √† garder en vue les DataNodes actifs et disponibles. 
      
- **(HDFS) Data Transfer Protocol**: variante du protocole TCP/IP et qui s'appuie sur celui-ci pour le transfert de donn√©es (binary code) / t√¢ches (bytecode) √† travers les noeuds du cluster Hadoop. 
  
- **Replication Protocol**:
	- Lors de l'√©criture de donn√©es, HDFS utilise un pipeline pour r√©pliquer les blocs entre les DataNodes. Ce processus de r√©plication suit un protocole sp√©cifique pour garantir l'int√©grit√© et la redondance des donn√©es.
	  
      ![alt text](img/cours1/block_replication.PNG)

- **Balancer Protocol**:
    - HDFS comprend un √©quilibreur qui redistribue les donn√©es entre les DataNodes afin de maintenir une distribution uniforme. Ce qui implique que plusieurs blocks ou partitions peuvent se retrouver sur le m√™me noeud esclave (slave node). 


#### 2.2.2) MapReduce üìå
MapReduce est la couche (par d√©faut) de traitement des donn√©es de Hadoop.
C'est aussi un mod√®le de programmation con√ßu pour traiter et transformer de grands volumes de donn√©es en parall√®le. L'id√©e √©tant que l'utilisateur d√©veloppe une application Hadoop en s'appuyant sur le mod√®le MapReduce qui permet ainsi la parall√©lisation des jobs de l'application sur un ensemble de machines. 

Une t√¢che MapReduce repose sur deux op√©rations fondamentales : 
- **Map**: la¬†fonction Map re√ßoit l‚Äôinput √† partir du disque¬†(d√©j√†) sous forme de paires "cl√©/valeur". Ces paires sont tri√©s et trait√©es, et un autre ensemble de cl√©/valeur interm√©diaire est produit.
- et **Reduce**: Apr√®s que tous les mappers aient termin√© leurs t√¢ches de traitement,¬†le framework m√©lange et organise¬†les r√©sultats (on appelle cela le shuffling). Les paires sont ensuite transmises aux "reducers". (Attention, un reducer ne peut d√©marrer si un mapper est encore actif.) La fonction Reduce re√ßoit donc aussi¬†les inputs sous forme de paires cl√©/valeur. Toutes les valeurs produites par map **ayant la m√™me cl√© sont assign√©es √† un reducer unique**. Celui-ci se charge d‚Äô**agr√©ger (somme, total, moyenne, ...) les valeurs pour cette cl√©**. Reduce produit ensuite un ouput final, toujours sous forme de paie cl√©/valeur.

![alt text](img/cours1/map_shuffle_reduce.PNG)

Chaque t√¢che travaillant sur une partie des donn√©es. Cela permet de r√©partir la charge de calcul sur l'ensemble des machines du cluster. L'id√©e est de d√©composer le job principal en t√¢ches (maps et reduces) qui peuvent √™tre ex√©cut√©es en parall√®le sur un sous-ensemble des donn√©es initiales. Donc ne s'applique que pour des jobs qui peuvent √™tre parall√©lis√©s. 
Souvent, ce ne sont pas les op√©rations MAP et REDUCE qui sont les plus difficiles √† concevoir mais la mani√®re de repr√©senter les donn√©es pour permettre d'appliquer facilement le mod√®le.

NB: A noter que les r√©sultats interm√©diaires tout comme finaux sont stock√©s sur disque ! (et pas en RAM comme avec Spark)

Un exemple classique est le programme WordCount (que nous verrons lors du TP1). La fonction Map √©met un 1 pour chaque mot rencontr√©. La fonction Reduce somme ces 1 pour chaque mot unique, donnant ainsi le compte total de chaque mot dans le texte.

#### 2.2.3) YARN üìå
YARN (Yet Another Resource Negotiator) est le gestionnaire de ressources et planificateur de t√¢ches du framework Hadoop. Il a √©t√© con√ßu pour s√©parer la gestion des ressources du traitement des donn√©es, offrant ainsi une plus grande flexibilit√© et efficacit√© dans l'utilisation des ressources du cluster. Yarn se charge de l'allocation de conteneurs ie de ressource pour le traitement et calcul (CPU et m√©moire). 

Yarn divise les t√¢ches de gestion des ressources et de planification/surveillance des travaux en 2 daemons distincts: 
- **ResourceManager**: il tourne sur le master node YARN et son r√¥le est d'arbitrer les ressources entre toutes les applications concurrentes du syst√®me (applications MapReduce ou Spark, Hive, Pig etc....). Il g√®re et alloue les ressources du cluster de mani√®re globale. C'est lui qui indique sur quel n≈ìud esclave faire tourner une application en fonction des disponibilit√©s du cluster. Il d√©cide aussi quelles applications ont priorit√© en cas de conflit de ressources.
- **NodeManager**: Il tourne sur les slave nodes YARN et son r√¥le est de surveiller et de rendre compte de l'utilisation des ressources par les divers conteneurs mis √† disposition pour faire tourner les applications soumises s'ex√©cutant sur le n≈ìud. 
  On a donc un NodeManager par slave node. Chaque conteneur (qui ne sont que des fractions des ressources telles que l'unit√© centrale, la m√©moire, le disque, le r√©seau, etc.) est isol√©, permettant √† des applications diff√©rentes de s'ex√©cuter sans interf√©rence.
  NodeManager -> ResourceManager: transmet des informations agr√©g√©es sur l'utilisation des ressources du n≈ìud.
  NodeManager -> ApplicationMaster: transmet des informations d√©taill√©es sur les conteneurs sp√©cifiques √† l'application.

Chaque application/Job (MapReduce, Spark, Hive etc...) se voit attribu√© un **ApplicationMaster** pendant la dur√©e d'ex√©cution du job. Une application √©tant un job ou un DAG (graphe compos√©e de plusieurs jobs s√©quenc√©s) de jobs. 
**L'applicationMaster est un processus cr√©e par le ResourceManager de YARN afin de g√©rer la n√©gociation des ressources** (avec le ResourceManager), mais aussi surveiller l'ex√©cution et red√©marrer un conteneur en cas d'incident de l'application concern√©e (en collaboration avec le NodeManager). Si un ApplicationMaster √©choue, le ResourceManager peut le relancer.

Vous remarquerez que le ResourceManager est le chef d'orchestre de tout cela, s'il crash, aucune application ne peut plus s'ex√©cuter sur le cluster. YARN dispose d'un mode dit "HA" (High Availability). C'est une configuration qui vise √† √©liminer le point unique de d√©faillance (Single Point Of Failure) dans un cluster Hadoop en permettant l'ex√©cution de multiples instances du ResourceManager. Pour cela, il s'appuie sur Apache Zookeeper. 


### 2.3) Zookeeper 

Apache ZooKeeper est un service centralis√© permettant de maintenir les informations de configuration, de nommage, de synchronisation distribu√©e et de services de groupe dans les syst√®mes distribu√©s.
Autrement dit, il assure la coordination distribu√©e n√©cessaire au mode HA de YARN, garantissant que le cluster reste op√©rationnel m√™me si les instances individuelles de ResourceManager tombent en panne. Il agit comme un service fiable et centralis√© pour maintenir l'√©tat partag√© critique et g√©rer l'√©lection du leader parmi les composants YARN.

![alt text](img/cours1/zookeeper.png)

1. Mode haute disponibilit√© (HA) :
   - Le mode HA garantit qu'un service reste op√©rationnel m√™me si l'une de ses instances tombe en panne.
   - Dans le contexte du YARN, il s'agit typiquement d'avoir plusieurs instances de ResourceManager pour √©viter un point de d√©faillance unique.

2. Comment ZooKeeper g√®re le HA :
   - ZooKeeper maintient l'√©tat du cluster et coordonne l'√©lection du leader.
   - Pour YARN HA :
     a. Plusieurs instances de ResourceManager sont configur√©es.
     b. ZooKeeper d√©termine quelle instance est active et laquelle est en attente.
     c. Si l'instance active tombe en panne, ZooKeeper facilite l'√©lection d'une nouvelle instance active parmi les instances en attente.
   - ZooKeeper utilise son protocole de consensus pour s'assurer que tous les n≈ìuds ont une vision coh√©rente du ResourceManager actif.

3. Interaction de ZooKeeper avec YARN :
   - ZooKeeper ne g√®re pas directement les processus YARN. Il fournit plut√¥t des services que YARN utilise pour l'AH :
     a. Stockage de l'√©tat du cluster
     b. √âlection du leader pour le ResourceManager
     c. Cl√¥ture pour √©viter les sc√©narios de cerveau divis√©



---
Sources:
- https://data-flair.training/blogs/hadoop-architecture/
- https://github.com/backstreetbrogrammer/11_JavaSpark
- https://www.scilife.io/blog/stages-data-lifecycle-management
- https://blog.bytebytego.com/
- https://www.qlik.com/us/data-replication/database-replication
- https://datascientest.com/mapreduce
- https://www.youtube.com/watch?v=WBrAPR5JyBw&ab_channel=RishiSrivastava