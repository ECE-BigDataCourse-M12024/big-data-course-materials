
# 1. Spark

### 1.1) Introduction et comparaison avec Hadoop (MapReduce) üìå

**A. Introduction:**

**Spark** est un :
- **framework, open-source** con√ßu pour le **traitement** distribu√© de calcul sur de grandes quantit√©s de donn√©es (Big Data) sur des clusters de machines. 
- Il s'appuie sur la mise en **cache en m√©moire**, et la cr√©ation et ex√©cution de **requ√™tes optimis√©es** pour traiter efficacement ces grosses volum√©tries de donn√©es. 
- Il fournit des API de d√©veloppement en **Java, Scala, Python et R**. 
- Il permet d'effectuer une grande vari√©t√© de t√¢ches: traitement par lots, requ√™tes interactives, analytique en temps r√©el,¬†machine learning¬†et traitement de graphes.

Apache Spark a √©t√© d√©velopp√© initialement en 2009 √† l'UC Berkeley's AMPLab dans le cadre d'un projet de recherche. L'objectif de Spark √©tait de cr√©er un nouveau cadre, optimis√© pour les **traitements it√©ratifs rapides** tels que le machine learning et **l'analyse interactive** des donn√©es, tout en conservant la capacit√© de mise √† l'√©chelle et la tol√©rance aux pannes de Hadoop MapReduce.
Il a √©t√© introduit pour la 1√®re fois en 2014 (1√®re release officielle) et aujourd'hui son d√©veloppement est maintenu par la fondation Apache Software Foundation. 


**B. Comparaison avec MapReduce:** 

1. **Performance** : Spark peut √™tre jusqu'√† 100 fois plus rapide que MapReduce pour certaines t√¢ches, gr√¢ce au traitement en m√©moire. En effet, MapReduce √©crit la plupart des donn√©es sur le disque apr√®s chaque op√©ration de map et de reduce, alors que Spark conserve la plupart des donn√©es en m√©moire apr√®s chaque transformation. Spark n'√©crit sur le disque que lorsque la m√©moire est pleine. Spark utilise aussi un syst√®me de caching, qui permet de conserver et r√©utiliser certaines donn√©es impliqu√©es dans plusieurs op√©rations. 
   ![alt text](img/cours2/hadoop_vs_spark_exec_flow.PNG)
   
2. **Polyvalent** : Spark supporte plusieurs types de traitements (batch, streaming, machine learning, graphes) dans un m√™me framework.
3. **Flexibilit√©**: S'int√®gre bien √† l'√©cosyst√®me Hadoop (et notamment YARN et HDFS), accepte une grande vari√©t√© de format pour le stockage: HDFS, AWS S3, Cassandra, HBase etc....
   ![alt text](img/cours2/flexibilite_spark.png)
   
4. **Facilit√© d'utilisation** : API plus riche et plus intuitive que MapReduce.
5. **Optimisation** : Spark optimise le plan d'ex√©cution avant de lancer les calculs. + Data Locality: en effet, Spark tente d'atteindre la localit√© des donn√©es en pr√©f√©rant planifier les t√¢ches sur les n≈ìuds o√π les donn√©es r√©sident d√©j√† (par exemple, les blocs HDFS). Cela permet de r√©duire les transferts sur le r√©seau (quand c'est possible).
6. **Interactivit√©** : Spark propose un shell (une interface de ligne de commande interactive pour ex√©cuter des commandes Spark) interactif pour l'exploration des donn√©es.


**C. Architecture de Spark et principales librairies:**

![alt text](img/cours2/SparkFramework.png)


L'architecture de Spark se compose de :
1. **Spark Core:** Spark Core est le moteur d'ex√©cution de la plateforme. Il est responsable de:
	- la gestion de la m√©moire (inclue gestion des RDDs), 
	- la r√©solution des pannes, 
	- la planification, la distribution et la surveillance des t√¢ches, 
	- ainsi que de l'interaction avec les syst√®mes de stockage. 
	  
	  Tout un syst√®me d'APIs s'appuie sur Spark Core pour exposer ses fonctionnalit√©s. Ces API cachent la complexit√© du traitement distribu√© derri√®re des op√©rateurs simples et de haut niveau.

2. **API Layers (Couches d'API):** 
	Spark fournit plusieurs couches d'API, toutes construites au-dessus de Spark Core :
	- API de bas niveau : API RDD (partie de Spark Core)
	- API de haut niveau : API DataFrame, API Dataset
	- API sp√©cifiques √† un langage : Scala, Java, Python (PySpark), R (SparkR)
	  
	Nos instructions s'appuient sur des librairies qui sont traduitent par les APIs sp√©cifiques. Celles-ci convertissent et communiquent les instructions au Spark Core, qui √† son tour va cr√©er et distribuer des op√©rations √† travers le cluster. 

3. **Les biblioth√®ques haut niveau:** 
	Construit au-dessus du Spark Core, Spark fournit 4 biblioth√®ques de plus haut niveau pour des t√¢ches sp√©ciales :
	- **Spark SQL** : Module pour le traitement de donn√©es structur√©es/semi-structur√©es. Pour cela il s'appuie sur une abstraction de donn√©es appel√©e DataFrames. Il permet aussi d'effectuer des requ√™tes interactives distribu√©es en s'appuyant sur du SQL ou HiveQL afin d'interroger les donn√©es stock√©es. 
	- **Spark Streaming** : Module pour l'acquisition et le traitement en temps r√©el (streaming). Il **ing√®re les donn√©es par mini-lots** et permet l'analytique de ces donn√©es √† l'aide du m√™me code d'application √©crit pour l'analytique par lots (batch processing). Cela am√©liore la productivit√© des d√©veloppeurs, car ils peuvent utiliser le m√™me code pour le traitement par lots et pour les applications de streaming en temps r√©el. Spark Streaming prend en charge les donn√©es de Twitter, Kafka, Flume, HDFS et ZeroMQ, ainsi que de nombreuses autres donn√©es issues de l'√©cosyst√®me Spark Packages.
	- **MLlib** : Biblioth√®que de machine learning permettant de parall√©liser les algorithmes de ML et de les appliquer sur des grosses volum√©tries de donn√©es r√©parties au sein d'un cluster.  Les algorithmes disponibles incluent la capacit√© de proc√©der √† la classification, √† la r√©gression, au clustering, au filtrage collaboratif et √† l'exploration de mod√®les.
	- **GraphX** : API pour le traitement de graphes. GraphX fournit un ETL, une analyse exploratoire et un calcul graphique it√©ratif pour permettre aux utilisateurs de cr√©er et de transformer de mani√®re interactive une structure de donn√©es de graphe √† grande √©chelle. Il est livr√© avec une API tr√®s flexible et une s√©lection d'algorithmes Graph distribu√©s.


### 1.2) Spark: concepts et fonctionnement üìå

#### 1.2.1) RDD (Resilient Distributed Datasets)

L'abstraction cl√© sur laquelle s'appuie Spark pour la repr√©sentation de donn√©e s'appelle: RDD. C'est l'unit√© fondamentale de donn√©es dans Spark. 
RDD est l'abr√©viation de : R√©silient, Distribu√©, Dataset
- **R√©silient** : car la redondance permet de reconstruire les donn√©es en cas de panne.
- **Distribu√©** : les donn√©es sont r√©parties sur plusieurs n≈ìuds du cluster.
- **Dataset** : repr√©sente la donn√©e manipul√©e. Celle-ci est partitionn√©e et est charg√©e en m√©moire √† partir d'un json, fichier texte, csv, base de donn√©e, HDFS etc...
  
Caract√©ristiques des RDD :
- **Immuables:** Les donn√©es d√©finies, r√©cup√©r√©es ou cr√©√©es ne peuvent pas √™tre modifi√©es une fois qu'elles sont enregistr√©es dans le syst√®me. Si nous devons acc√©der √† un RDD existant ou le modifier, nous devons cr√©er un nouveau RDD en appliquant un ensemble de fonctions de transformation au RDD actuel ou pr√©c√©dent.
- **√âvalu√©s paresseusement (lazy evaluation):** cela signifie que sont enregistr√©es les transformations √† effectuer, mais que ceux-ci ne sont appliqu√©s pour cr√©er le r√©sultat uniquement sur demande (lors d'une action cf la partie sur actions vs transformations). 
- Peuvent √™tre **mis en cache en m√©moire** et r√©utilisable.

#### 1.2.2) Dataframes & Datasets 

Les DataFrames et Datasets sont des abstractions introduites par la suite afin d'apporter plus de flexibilit√©, performance et fonctionnalit√©s aux RDD. Elles sont de plus haut niveau que les RDD et s'appuient sur celles-ci (donc b√©n√©ficie des caract√©ristiques des RDDs). 
Ces abstraction (via leur API respective) sont accessibles aux API sp√©cifiques √† un langage (Python, R, Java, Scala).

![alt text](img/cours2/dataset_and_dataframe_in_architecture.PNG)

-**DataFrames** :
Ce sont des collections distribu√©es de donn√©es organis√©es en colonnes nomm√©es.
- Similaires √† une table dans une base de donn√©es relationnelle ou dataframe dans R/Python.
- Optimis√©s pour les performances gr√¢ce √† un 'query optimizer' (Catalyst Optimizer).
Le DataFrame contient des lignes avec un sch√©ma. Le sch√©ma indique la structure des donn√©es.

![alt text](img/cours2/spark_schema.png)

-**Datasets** :
- Extension typ√©e des DataFrames (disponible uniquement en Scala et Java, en effet Python et R sont dit 'non typ√©s', typage dynamique).
- Combine les avantages des RDD (typage fort) et des DataFrames (optimisations).


Comparaison :

| Aspect       | RDD           | DataFrame     | Dataset     |
| ------------ | ------------- | ------------- | ----------- |
| Typage       | Fort          | Faible        | Fort        |
| Optimisation | Manuelle      | Automatique   | Automatique |
| Sch√©ma       | Non           | Oui           | Oui         |

- **Principales √©volutions** :
  - 2014 : Spark 1.0, introduction de Spark SQL
  - 2015 : Spark 1.3, introduction des DataFrames
  - 2016 : Spark 2.0, unification de l'API DataFrame/Dataset
  - 2019 : Spark 3.0, am√©lioration des performances et support de Kubernetes


#### 1.2.3) Op√©rations:

**Transformations** :
- Op√©rations qui cr√©ent un nouveau RDD √† partir de RDD existant.
- √âvalu√©es paresseusement (lazy).
- Exemples : map(), filter(), groupBy(), union(), reduceBy().

>![alt text](img/cours2/transformation_examples.PNG)


**Actions** :
- Op√©rations qui d√©clenchent le calcul et retournent une valeur ou √©crivent des donn√©es.
- Exemples : count(), collect(), save(), first().
  Les r√©sultats des actions sont stock√©s sur la machine qui run (ex√©cute) g√©n√©ralement le driver (donc attention au niveau de la gestion m√©moire!). Plus exactement, le r√©sultat est collect√© √† partir des ex√©cuteurs et est envoy√© au driver. 

>![alt text](img/cours2/actions_example.PNG)


![alt text](img/cours2/RDDOperations.png)



### 1.3) Anatomie d'une application Spark

#### 1.3.1) Principaux composants d'une application Spark üìå

**Composants du cluster** :
- Cluster Manager : G√®re les ressources du cluster.
- Worker Nodes : Noeuds du cluster qui ex√©cutent les t√¢ches.
- Executors : Processus JVM (de Spark) qui tournent sur les Worker Nodes.

![alt text](img/cours2/spark_composants.PNG)

**Driver Program** :
C'est l'√©quivalent de votre programme (√©crit en Java/Python/Scala/R) qui va se lancer sous forme de **processus JVM**. C'est le processus qui marque la cr√©ation de l'application Spark. C'est le point d'entr√©e pour interagir avec le Spark-core et les librairies Spark.
Son r√¥le: 
- Contient la m√©thode `main()`.
- Cr√©e/G√©n√®re le SparkContext/SparkSession.
- D√©finit les op√©rations √† effectuer sur les donn√©es.

**SparkContext** :
C'est un objet (une instance de classe) qui **d√©fini et fourni le cadre, le contexte de travail pour notre application Spark** (via le SparkConf on d√©finit par exemple, le nombre, la taille de la m√©moire et les c≈ìurs utilis√©s par l'ex√©cuteur s'ex√©cutant sur les n≈ìuds de travail/worker nodes). 
Il encapsule la connexion au cluster Spark et fournit l'API principale (Spark-core) permettant de travailler avec les RDDs et de b√©n√©ficier des fonctionnalit√©s de traitement distribu√© de Spark. A cette fin il s'appuie **sur un ensemble d'objets (DAGScheduler, TaskScheduler, SchedulerBackEnd, SparkEnv) qu'il cr√©e et manage**. 

![alt text](img/cours2/spark-context.PNG)

- DAGScheduler: 
	- G√©n√®re le DAG: un plan d'ex√©cution des op√©rations optimis√© (en regroupant les transformations, minimisant le shuffle)
	- D√©coupe le job spark en 'stages' (√©tapes) et t√¢ches qu'il soumet au TaskScheduler
- TaskScheduler:
	- Responsable de l'envoi des t√¢ches au cluster et de leur ordonnancement sur les ex√©cuteurs (via le SchedulerBackend).
- SchedulerBackend: 
	- Fournit une interface entre le TaskScheduler et le gestionnaire de cluster (cluster manager) sous-jacent (par exemple, YARN, Mesos). Il traduit les demandes de ressources de Spark et les commandes de lancement de t√¢ches via le protocole utilis√© par le gestionnaire de cluster.
- SparkEnv:
	- Contient des objets d'environnement d'ex√©cution tels que le s√©rialiseur, l'environnement RPC (pour la communication driver <-> executors), etc.



**SparkSession** :
Avant Spark 2.0, les points d'entr√©e pour les applications Spark comprenaient :
- le SparkContext, utilis√© pour les applications Spark core, 
- le SQLContext et le HiveContext, utilis√©s avec les applications Spark SQL, 
- et le StreamingContext, utilis√© pour les applications Spark Streaming. 
  
L'objet SparkSession introduit avec Spark 2.0 **combine tous ces objets en un seul point d'entr√©e** qui peut √™tre utilis√© pour toutes les applications Spark.

![alt text](img/cours2/spark_components_and_archi.PNG)

>NB: Il est important d'appeler un stop() lorsque vous avez termin√© votre application Spark, afin de s'assurer que les ressources sont correctement lib√©r√©es et que l'application Spark se termine de mani√®re √©l√©gante. (Cela arr√™tera donc les processus JVMs libr√©rant les ressources pour d'autres applications).

#### 1.3.2)  Fonctionnement global

R√©sum√© des √©tapes de communication entre Spark et un gestionnaire de cluster :

1. Initialisation :
   - Le Driver Program de Spark (au moment de la cr√©ation du SparkContext) initie la communication en demandant des ressources au Cluster Manager.
2. Allocation des ressources :
   - Le Cluster Manager alloue des ressources et lance des Executors sur les Worker Nodes.
3. Enregistrement des Executors :
   - Les Executors s'enregistrent aupr√®s du Driver, √©tablissant une communication bidirectionnelle.

4. Cr√©ation, distribution et ex√©cution des t√¢ches :
   - Le Driver g√©n√®re le plan d'ex√©cution et les t√¢ches via le DAGScheduler, 
   - Distribue les t√¢ches aux Executors via le TaskScheduler et le SchedulerBackend.
   - Les Executors ex√©cutent les t√¢ches et rapportent leur √©tat et leurs r√©sultats au Driver.

![alt text](img/cours2/spark_in_details.PNG)

5. Gestion dynamique des ressources :
   - Le Driver peut demander dynamiquement plus ou moins de ressources au Cluster Manager selon les besoins, si cette fonctionnalit√© est activ√©e.

6. Protocoles de communication :
   - La communication utilise principalement RPC (Remote Procedure Call) entre le Driver et les Executors.
   - La communication avec le Cluster Manager d√©pend du type de gestionnaire utilis√© :
     * Pour YARN : Protocoles sp√©cifiques √† YARN
     * Pour Kubernetes : API Kubernetes
     * Pour le mode standalone : Protocoles internes de Spark


#### 1.3.3) Modes üìå

Deux modes de d√©ploiement peuvent √™tre utilis√©s pour soumettre des applications Spark √† un cluster : le mode client et le mode cluster. 

**Client mode** :
- Le driver s'ex√©cute **sur la machine cliente.**
- Utile pour le d√©veloppement et le d√©bogage (car si le driver est ko, c'est toute l'application qui √©choue).
- Le mode client est pris en charge √† la fois pour les sessions shell interactives (pyspark, spark-shell, etc.) et pour la soumission d'applications non interactives.
- L'UI de l'application est disponible sur http://localhost:4040 en mode local.

![alt text](img/cours2/spark_client_mode.PNG)

**Cluster mode** :
- Le driver s'ex√©cute **sur un des n≈ìuds du cluster.**
- Recommand√© pour la production (car le driver lui-m√™me s'ex√©cute sur le cluster en tant que sous-processus de l'ApplicationMaster. Cela permet d'assurer la r√©silience : Si le processus ApplicationMaster h√©bergeant le driver tombe en panne, il peut √™tre ex√©cut√© par un autre processus.)
- Meilleure tol√©rance aux pannes.

![alt text](img/cours2/spark_cluster_mode.PNG)

Spark est agnostique par rapport au gestionnaire de cluster sous-jacent. Tant qu'il peut acqu√©rir des processus d'ex√©cution et que ceux-ci communiquent entre eux, il est relativement facile de l'ex√©cuter m√™me sur un gestionnaire de cluster qui prend √©galement en charge d'autres applications (par exemple Mesos/YARN/Kubernetes).

**Compl√©ment sur les modes de d√©ploiement** :
- **Local** : Tout s'ex√©cute sur une seule machine.
- **Standalone** : Cluster Spark g√©r√© par Spark lui-m√™me. A savoir que le cluster Manager de Spark s'appelle le Spark Master. 
- **YARN** : Utilise le gestionnaire de ressources de Hadoop.
- **Mesos** : Utilise Apache Mesos comme gestionnaire de ressources.
- **Kubernetes** : D√©ploiement sur un cluster Kubernetes.

Plus d'informations sur comment connect√© Spark avec un autre gestionnaire/n√©gociateur de ressources:
https://spark.apache.org/docs/latest/submitting-applications.html#master-urls

![alt text](img/cours2/spark_master_setup.PNG)

Nb: 
>Etant donn√© que le driver planifie les t√¢ches sur le cluster, en pratique on pr√©f√®re l'ex√©cuter √† proximit√© des workers nodes, id√©alement sur le m√™me r√©seau local.
>Si nous souhaitons envoyer des requ√™tes au cluster √† distance, il est pr√©f√©rable de d√©ployer Spark en mode Cluster, ce qui va ouvrir une connexion RPC vers le driver qui pourra soumettre les op√©rations et t√¢ches au plus pr√®s des workers. 


---
cheatsheet: 
- https://github.com/JeroenSchmidt/CheetSheets/blob/master/Spark/Running%20Spark%20on%20a%20Cluster.md
- https://intellipaat.com/mediaFiles/2019/03/spark-and-rdd-cheat-sheet-1.png

sources:
- https://moazim1993.github.io/BigData_Spark_Tutorial/
- https://aws.amazon.com/fr/what-is/apache-spark/
- https://www.infoworld.com/article/2257062/tutorial-spark-application-architecture-and-clusters.html
